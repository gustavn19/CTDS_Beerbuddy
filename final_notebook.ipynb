{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Beer Recommender System\n",
    "This notebook implements data preprocessing and modeling techniques to create a beer recommender system. I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\agent\\anaconda3\\Lib\\site-packages\\torchvision\\datapoints\\__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "c:\\Users\\agent\\anaconda3\\Lib\\site-packages\\torchvision\\transforms\\v2\\__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n"
     ]
    }
   ],
   "source": [
    "#Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from collections import defaultdict\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing Functions\n",
    "These functions clean the dataset by handling duplicates, missing values, and incorrect formats. They prepare the data for splitting and analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(df):\n",
    "    df_filtered = df.drop_duplicates([\"name\", \"reviewer\", \"review_text\"]) # Remove duplicate entries\n",
    "    print(\"Size after drop_duplicates: \", len(df_filtered))\n",
    "    \n",
    "    df_filtered['rating'] = pd.to_numeric(df_filtered['rating'], errors='coerce')  # Set erros to NaN\n",
    "    df_filtered = df_filtered.dropna(subset=['rating'])  # Drop rows where 'rating' is NaN\n",
    "    print(\"Size after drop rating NA: \", len(df_filtered))\n",
    "    \n",
    "    df_filtered['abv'] = pd.to_numeric(df_filtered['abv'].str.rstrip('%'), errors='coerce') \n",
    "    df_filtered = df_filtered.dropna(subset=['abv'])\n",
    "    print(\"Size after drop abv NA: \", len(df_filtered))\n",
    "    \n",
    "    return df_filtered\n",
    "\n",
    "def create_test_train(df, reviewer_col=\"reviewer\", random_state=7, test_size=100, mask_percentage=0.10):\n",
    "    \"\"\"\n",
    "    Splits a dataset into training and test sets, masking a portion of test set entries.\n",
    "    \n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): The dataset to split.\n",
    "    - reviewer_col (str): The column name containing reviewer IDs.\n",
    "    - random_state (int): The random state for reproducibility.\n",
    "    - test_size (int): The number of reviewers to sample for the test set.\n",
    "    - mask_percentage (float): The percentage of beers to mask for each reviewer in the test set.\n",
    "\n",
    "    Returns:\n",
    "    - df_train (pd.DataFrame): The training set.\n",
    "    - df_test_masked (pd.DataFrame): The test set with masked entries.\n",
    "    \"\"\"\n",
    "    # Randomly sample reviewers\n",
    "    sampled_reviewers = df[reviewer_col].sample(n=test_size, random_state=random_state)\n",
    "    \n",
    "    # Get reviews from the sampled reviewers\n",
    "    df_test = df[df[reviewer_col].isin(sampled_reviewers)]\n",
    "    \n",
    "    # Group by reviewer to get each user's beers\n",
    "    df_test_grouped = df_test.groupby(reviewer_col)\n",
    "    \n",
    "    # Randomly mask a percentage of beers for each reviewer\n",
    "    test_set_masked = []\n",
    "    for reviewer, group in df_test_grouped:\n",
    "        # Calculate how many beers to mask\n",
    "        num_to_mask = max(int(len(group) * mask_percentage), 1)\n",
    "        \n",
    "        # Sample the calculated number of beers\n",
    "        masked_group = group.sample(n=num_to_mask, random_state=random_state)\n",
    "        test_set_masked.append(masked_group)\n",
    "    \n",
    "    # Combine masked reviews into a single DataFrame\n",
    "    df_test_masked = pd.concat(test_set_masked)\n",
    "    \n",
    "    # Remove masked reviews from the training data\n",
    "    df_train = df.drop(df_test_masked.index)\n",
    "    \n",
    "    # Display dataset summaries\n",
    "    print(\"\\n### Dataset Summary ###\")\n",
    "    print(f\"Total reviewers sampled: {len(sampled_reviewers)}\")\n",
    "    print(f\"Training set size: {df_train.shape}\")\n",
    "    print(f\"Test set size: {df_test_masked.shape}\")\n",
    "    \n",
    "    return df_train, df_test_masked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size after drop_duplicates:  1157819\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\agent\\AppData\\Local\\Temp\\ipykernel_4640\\3114772741.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_filtered['rating'] = pd.to_numeric(df_filtered['rating'], errors='coerce')  # Set erros to NaN\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size after drop rating NA:  1157807\n",
      "Size after drop abv NA:  1154739\n"
     ]
    }
   ],
   "source": [
    "# Load data and preprocess\n",
    "df = pd.read_pickle('encoded_beers_SBERT.pkl')\n",
    "\n",
    "df_filtered = preprocess_data(df)\n",
    "del df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>brewery</th>\n",
       "      <th>subgenre</th>\n",
       "      <th>abv</th>\n",
       "      <th>location</th>\n",
       "      <th>rating</th>\n",
       "      <th>average_rating</th>\n",
       "      <th>reviewer</th>\n",
       "      <th>review_date</th>\n",
       "      <th>review_text</th>\n",
       "      <th>algorithm_rating</th>\n",
       "      <th>total_reviews</th>\n",
       "      <th>sbert_embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Wild Dog Pale Ale</td>\n",
       "      <td>Wild Dog (Tiemann Beer)</td>\n",
       "      <td>American Pale Ale</td>\n",
       "      <td>5.2</td>\n",
       "      <td>ðŸ‡¯ðŸ‡ªJersey</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2.99</td>\n",
       "      <td>Jerseyislandbeer</td>\n",
       "      <td>December 14, 2023</td>\n",
       "      <td>330ml can from Shoprite in Livingstone. At hom...</td>\n",
       "      <td>28.0</td>\n",
       "      <td>11</td>\n",
       "      <td>[0.037878353, 0.00593541, 0.0062317043, -0.011...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Wild Dog Pale Ale</td>\n",
       "      <td>Wild Dog (Tiemann Beer)</td>\n",
       "      <td>American Pale Ale</td>\n",
       "      <td>5.2</td>\n",
       "      <td>ðŸ‡¬ðŸ‡§Ipswich, England</td>\n",
       "      <td>3.2</td>\n",
       "      <td>2.99</td>\n",
       "      <td>Grumbo</td>\n",
       "      <td>February 28, 2022</td>\n",
       "      <td>18/2/2022. Can sample courtesy of fonefan, che...</td>\n",
       "      <td>28.0</td>\n",
       "      <td>11</td>\n",
       "      <td>[-0.037820198, -0.044825517, 0.07764052, 0.065...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Wild Dog Pale Ale</td>\n",
       "      <td>Wild Dog (Tiemann Beer)</td>\n",
       "      <td>American Pale Ale</td>\n",
       "      <td>5.2</td>\n",
       "      <td>ðŸ‡¸ðŸ‡ªTyresÃ¶, Sweden</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2.99</td>\n",
       "      <td>omhper</td>\n",
       "      <td>February 19, 2022</td>\n",
       "      <td>--Sample, thanks fonefan! -- Hazy deep golden,...</td>\n",
       "      <td>28.0</td>\n",
       "      <td>11</td>\n",
       "      <td>[0.056960188, -0.00059301173, 0.11057871, 0.02...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Wild Dog Pale Ale</td>\n",
       "      <td>Wild Dog (Tiemann Beer)</td>\n",
       "      <td>American Pale Ale</td>\n",
       "      <td>5.2</td>\n",
       "      <td>ðŸ‡«ðŸ‡®Vasa, Finland</td>\n",
       "      <td>2.8</td>\n",
       "      <td>2.99</td>\n",
       "      <td>oh6gdx</td>\n",
       "      <td>January 31, 2022</td>\n",
       "      <td>Panda from a can, thanks fonefan!. Golden colo...</td>\n",
       "      <td>28.0</td>\n",
       "      <td>11</td>\n",
       "      <td>[0.003549767, -0.010705345, 0.02083684, 0.0106...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>Wild Dog Pale Ale</td>\n",
       "      <td>Wild Dog (Tiemann Beer)</td>\n",
       "      <td>American Pale Ale</td>\n",
       "      <td>5.2</td>\n",
       "      <td>ðŸ‡©ðŸ‡°Haderslev, Denmark</td>\n",
       "      <td>2.6</td>\n",
       "      <td>2.99</td>\n",
       "      <td>martin00sr</td>\n",
       "      <td>January 8, 2022</td>\n",
       "      <td>Can @Ulfborg. Cloudy amber, white head. Malty ...</td>\n",
       "      <td>28.0</td>\n",
       "      <td>11</td>\n",
       "      <td>[-0.01005388, -0.02942978, 0.0016338513, 0.017...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id               name                  brewery           subgenre  abv  \\\n",
       "0   1  Wild Dog Pale Ale  Wild Dog (Tiemann Beer)  American Pale Ale  5.2   \n",
       "1   2  Wild Dog Pale Ale  Wild Dog (Tiemann Beer)  American Pale Ale  5.2   \n",
       "2   3  Wild Dog Pale Ale  Wild Dog (Tiemann Beer)  American Pale Ale  5.2   \n",
       "3   4  Wild Dog Pale Ale  Wild Dog (Tiemann Beer)  American Pale Ale  5.2   \n",
       "4   6  Wild Dog Pale Ale  Wild Dog (Tiemann Beer)  American Pale Ale  5.2   \n",
       "\n",
       "               location  rating  average_rating          reviewer  \\\n",
       "0              ðŸ‡¯ðŸ‡ªJersey     3.5            2.99  Jerseyislandbeer   \n",
       "1    ðŸ‡¬ðŸ‡§Ipswich, England     3.2            2.99            Grumbo   \n",
       "2      ðŸ‡¸ðŸ‡ªTyresÃ¶, Sweden     3.5            2.99            omhper   \n",
       "3       ðŸ‡«ðŸ‡®Vasa, Finland     2.8            2.99            oh6gdx   \n",
       "4  ðŸ‡©ðŸ‡°Haderslev, Denmark     2.6            2.99        martin00sr   \n",
       "\n",
       "         review_date                                        review_text  \\\n",
       "0  December 14, 2023  330ml can from Shoprite in Livingstone. At hom...   \n",
       "1  February 28, 2022  18/2/2022. Can sample courtesy of fonefan, che...   \n",
       "2  February 19, 2022  --Sample, thanks fonefan! -- Hazy deep golden,...   \n",
       "3   January 31, 2022  Panda from a can, thanks fonefan!. Golden colo...   \n",
       "4    January 8, 2022  Can @Ulfborg. Cloudy amber, white head. Malty ...   \n",
       "\n",
       "  algorithm_rating  total_reviews  \\\n",
       "0             28.0             11   \n",
       "1             28.0             11   \n",
       "2             28.0             11   \n",
       "3             28.0             11   \n",
       "4             28.0             11   \n",
       "\n",
       "                                     sbert_embedding  \n",
       "0  [0.037878353, 0.00593541, 0.0062317043, -0.011...  \n",
       "1  [-0.037820198, -0.044825517, 0.07764052, 0.065...  \n",
       "2  [0.056960188, -0.00059301173, 0.11057871, 0.02...  \n",
       "3  [0.003549767, -0.010705345, 0.02083684, 0.0106...  \n",
       "4  [-0.01005388, -0.02942978, 0.0016338513, 0.017...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_filtered.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Dataset Summary ###\n",
      "Total reviewers sampled: 100\n",
      "Training set size: (1149910, 14)\n",
      "Test set size: (4829, 14)\n"
     ]
    }
   ],
   "source": [
    "df_train, df_test_masked = create_test_train(df_filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create locality-sensitive hashing (LSH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a variable to store the model\n",
    "sbert_model = None\n",
    "\n",
    "def encode_sbert(query, model_name='sentence-transformers/all-MiniLM-L6-v2'):\n",
    "    \"\"\"\n",
    "    Encodes a query using SBERT. Loads the model if not already loaded.\n",
    "    \n",
    "    Parameters:\n",
    "        query (str or list of str): The query or list of queries to encode.\n",
    "        model_name (str): The name of the SBERT model to load (default is 'all-MiniLM-L6-v2').\n",
    "    \n",
    "    Returns:\n",
    "        numpy.ndarray: The embedding(s) for the input query/queries.\n",
    "    \"\"\"\n",
    "    global sbert_model  # Use the global variable to store the model\n",
    "    \n",
    "    # Load the model if it's not already loaded\n",
    "    if sbert_model is None:\n",
    "        sbert_model = SentenceTransformer(model_name)\n",
    "    \n",
    "    # Encode the query and return the embeddings\n",
    "    return sbert_model.encode(query)\n",
    "\n",
    "def generate_hyperplanes(dim, num_hash_functions):\n",
    "    \"\"\"\n",
    "    Generate random hyperplanes for hash functions.\n",
    "    \n",
    "    Parameters:\n",
    "    - dim: Dimensionality of the embeddings.\n",
    "    - num_hash_functions: Number of hash functions per table.\n",
    "    \n",
    "    Returns:\n",
    "    - A matrix of shape (num_hash_functions, dim) where each row is a hyperplane.\n",
    "    \"\"\"\n",
    "    return np.random.randn(num_hash_functions, dim)\n",
    "\n",
    "def hash_vectors(vectors, hyperplanes):\n",
    "    \"\"\"\n",
    "    Hash a batch of vectors using a set of hyperplanes.\n",
    "\n",
    "    Parameters:\n",
    "    - vectors: Input vectors (2D array of shape [n_samples, d]).\n",
    "    - hyperplanes: Matrix of hyperplanes (2D array of shape [k, d]).\n",
    "\n",
    "    Returns:\n",
    "    - A matrix of binary hash values (shape [n_samples, k]).\n",
    "    \"\"\"\n",
    "    # Compute dot products and return binary hash values\n",
    "    return (np.dot(vectors, hyperplanes.T) > 0).astype(int)\n",
    "\n",
    "class LSHVectorized:\n",
    "    def __init__(self, d, k, L):\n",
    "        \"\"\"\n",
    "        Initialize the LSH scheme with vectorized support.\n",
    "\n",
    "        Parameters:\n",
    "        - d: Dimensionality of the input vectors.\n",
    "        - k: Number of hash functions per table.\n",
    "        - L: Number of hash tables.\n",
    "        \"\"\"\n",
    "        self.L = L\n",
    "        self.tables = [defaultdict(list) for _ in range(L)]\n",
    "        self.hyperplanes = [generate_hyperplanes(d, k) for _ in range(L)]\n",
    "\n",
    "    def add_vectors(self, vectors, identifiers):\n",
    "        \"\"\"\n",
    "        Add a batch of vectors to the LSH index.\n",
    "\n",
    "        Parameters:\n",
    "        - vectors: Input vectors (2D array of shape [n_samples, d]).\n",
    "        - identifiers: A list of unique identifiers for the vectors.\n",
    "        \"\"\"\n",
    "        for table, hyperplanes in zip(self.tables, self.hyperplanes):\n",
    "            # Compute hash values for all vectors at once\n",
    "            hash_values = hash_vectors(vectors, hyperplanes)\n",
    "            \n",
    "            # Convert binary hash values to tuples for dictionary keys\n",
    "            hash_keys = [tuple(h) for h in hash_values]\n",
    "            \n",
    "            # Add vectors to their corresponding buckets\n",
    "            for identifier, key in zip(identifiers, hash_keys):\n",
    "                table[key].append(identifier)\n",
    "\n",
    "    def query(self, vectors):\n",
    "        \"\"\"\n",
    "        Query the LSH index to find similar items for a batch of vectors.\n",
    "\n",
    "        Parameters:\n",
    "        - vectors: Query vectors (2D array of shape [n_samples, d]).\n",
    "\n",
    "        Returns:\n",
    "        - A list of sets, where each set contains the candidates for a query vector.\n",
    "        \"\"\"\n",
    "        candidates = [set() for _ in range(len(vectors))]\n",
    "        for table, hyperplanes in zip(self.tables, self.hyperplanes):\n",
    "            # Compute hash values for all query vectors\n",
    "            hash_values = hash_vectors(vectors, hyperplanes)\n",
    "            \n",
    "            # Convert binary hash values to tuples for dictionary keys\n",
    "            hash_keys = [tuple(h) for h in hash_values]\n",
    "            \n",
    "            # Retrieve candidates for each query\n",
    "            for i, key in enumerate(hash_keys):\n",
    "                candidates[i].update(table.get(key, []))\n",
    "        return candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = np.vstack(df_filtered[\"sbert_embedding\"].values)  # Combine embeddings into a 2D array\n",
    "identifiers = df_filtered.index.tolist()  # Use review IDs as identifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run LSH ##\n",
    "# Initialize LSH scheme\n",
    "d = 384\n",
    "k = 14 \n",
    "L = 7\n",
    "\n",
    "lsh = LSHVectorized(d, k, L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add vectors to the LSH index\n",
    "lsh.add_vectors(vectors, identifiers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make reccomendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def predict_ratings_user_based(user_item_matrix, similarity_matrix):\n",
    "    # TODO Hvordan hÃ¥ndterer vi Ã¸l personen allerede har rated\n",
    "    # Convert to a numpy array for computation\n",
    "    user_item_matrix = user_item_matrix.values\n",
    "\n",
    "    # Compute mean ratings for each user\n",
    "    user_means = np.ma.masked_equal(user_item_matrix, 0).mean(axis=1).filled(0)\n",
    "    \n",
    "    # Center the matrix by subtracting user means\n",
    "    ratings_diff = user_item_matrix - user_means[:, None]\n",
    "    ratings_diff[np.isnan(ratings_diff)] = 0  # Replace NaN deviations with 0\n",
    "\n",
    "    # Compute predictions\n",
    "    similarity_sum = np.abs(similarity_matrix).sum(axis=1)[:, None]\n",
    "    pred = user_means[:, None] + np.dot(similarity_matrix, ratings_diff) / (similarity_sum + 1e-8)\n",
    "    \n",
    "    return pred\n",
    "\n",
    "def collaborative_filtering(df_user):\n",
    "    \n",
    "    user_matrix = df_user.pivot_table(\n",
    "        index=\"reviewer\",     # Rows: Reviewers\n",
    "        columns=\"name\",       # Columns: Beer names\n",
    "        values=\"rating\",      # Values: Ratings\n",
    "        fill_value=0          # Fill missing ratings with 0\n",
    "    )\n",
    "    \n",
    "    # Compute cosine similarity\n",
    "    cosine_similarity_matrix = cosine_similarity(user_matrix)\n",
    "    \n",
    "    # Predict ratings\n",
    "    predicted_ratings = predict_ratings_user_based(user_matrix, cosine_similarity_matrix)\n",
    "\n",
    "    df_out = pd.DataFrame(predicted_ratings, index=user_matrix.index, columns=user_matrix.columns)\n",
    "\n",
    "\n",
    "    return df_out\n",
    "\n",
    "collab_df = collaborative_filtering(df_filtered)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 recommended beers:\n",
      "                                        score                       beer\n",
      "beer                                                                    \n",
      "Schneider Weisse Tap 06 - Aventinus  0.649334  Firestone Walker Parabola\n",
      "Unibroue Trois Pistoles              0.599655             Brooklyn Lager\n",
      "Ayinger Celebrator Doppelbock        0.586081   Hitachino Nest White Ale\n",
      "Trappistes Rochefort 10              0.559206             Maine Beer Zoe\n",
      "Trappistes Rochefort 10              0.559206             Maine Beer Zoe\n",
      "\n",
      "Top flavor-related terms:\n",
      "['dark', 'flavor', 'sweet']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\agent\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1382: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=7.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import StandardScaler, normalize\n",
    "from sklearn.decomposition import PCA\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Define reference context for flavor-related words. The first 20 is from a aromatic kit used for sommeliers, the rest is ai-generated.\n",
    "context_words = [\n",
    "    \"bitter\", \"sweet\", \"salt\", \"sour\", \"umami\",\n",
    "    \"lemon\", \"grapefruit\", \"apple\", \"pear\", \"blackcurrant\", \"prune\", \"melon\", \n",
    "    \"banana\", \"acacia\", \"rose\", \"cut grass\", \"hay\", \"bay leaf\", \"thyme\", \n",
    "    \"tomato\", \"pepper\", \"nutmeg\", \"clove\", \"bread\", \"butter\", \"vanilla\", \n",
    "    \"hazelnut\", \"toast\", \"malt\", \"caramel\", \"honey\", \"coffee\", \"licorice\",\n",
    "    \"pine\", \"grass\", \"resin\", \"floral\", \"perfume\", \"incense\", \"cinnamon\",\n",
    "    \"ginger\", \"anise\", \"nut\", \"almond\", \"walnut\", \"chestnut\", \"peanut\",\n",
    "    \"soy\", \"mushroom\", \"earth\", \"dust\", \"wood\", \"barnyard\", \"horse\",\n",
    "    \"wet\", \"dry\", \"metallic\", \"sulfur\", \"fish\", \"cheese\", \"butter\",\n",
    "    \"cream\", \"leather\", \"silk\", \"rubber\", \"barnyard\", \"ammonia\",\n",
    "    \"rotten\", \"acid\"\n",
    "]\n",
    "custom_stop_words = [\"beer\", \"beers\", \"bottle\", \"taste\", \"nice\", \"aroma\", \"like\", \"good\", \"great\", \"head\"]\n",
    "context_embeddings = encode_sbert(context_words)\n",
    "\n",
    "# Function to filter terms dynamically\n",
    "def is_flavor_related(term, context_embeddings, threshold=0.35):\n",
    "    term_embedding = sbert_model.encode([term])[0]\n",
    "    cosine_similarity = lambda x, y: np.dot(x, y) / (np.linalg.norm(x) * np.linalg.norm(y))\n",
    "    max_similarity = max(cosine_similarity(term_embedding, context) for context in context_embeddings)\n",
    "    return max_similarity > threshold\n",
    "\n",
    "def plot_bucket(bucket_vectors, cluster_labels, subgenres, perplexity=30, n_iter=5000, learning_rate=200):\n",
    "    \"\"\"\n",
    "    Visualizes differences within an LSH bucket using t-SNE with configurable parameters.\n",
    "    \n",
    "    Args:\n",
    "        bucket_vectors (np.ndarray): High-dimensional vectors of beers in the bucket.\n",
    "        cluster_labels (np.ndarray): Cluster labels assigned to each vector.\n",
    "        subgenres (np.ndarray): Subgenre or categorical labels for each beer.\n",
    "        perplexity (int): The t-SNE perplexity parameter, balancing local/global data views.\n",
    "        n_iter (int): Number of iterations for t-SNE optimization.\n",
    "        learning_rate (float): Learning rate for t-SNE optimization.\n",
    "    \"\"\"\n",
    "    # t-SNE reducer with tuned parameters\n",
    "    reducer = TSNE(\n",
    "        n_components=2, \n",
    "        random_state=42, \n",
    "        perplexity=perplexity, \n",
    "        n_iter=n_iter, \n",
    "        learning_rate=learning_rate\n",
    "    )\n",
    "    reduced_vectors = reducer.fit_transform(bucket_vectors)\n",
    "    \n",
    "    # Plotting\n",
    "    plt.figure(figsize=(16, 10))\n",
    "    scatter = plt.scatter(\n",
    "        reduced_vectors[:, 0],\n",
    "        reduced_vectors[:, 1],\n",
    "        c=cluster_labels,\n",
    "        cmap='plasma',\n",
    "        alpha=0.7\n",
    "    )\n",
    "    plt.colorbar(scatter, label='Cluster Label')\n",
    "    plt.title(f\"t-SNE Visualization (Perplexity={perplexity}, n_iter={n_iter}, LR={learning_rate})\")\n",
    "    plt.xlabel(\"Component 1\")\n",
    "    plt.ylabel(\"Component 2\")\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "def recommend_beer(query, user_name, abv_desired, n_clusters=15):\n",
    "    # Encode the query\n",
    "    query_embedding = encode_sbert(query).reshape(1, -1)\n",
    "    \n",
    "    # Query the LSH index\n",
    "    candidates = lsh.query(query_embedding)\n",
    "\n",
    "    # Filter bucket vectors and metadata\n",
    "    bucket_data = df_filtered[df_filtered[\"id\"].isin(list(candidates[0]))]\n",
    "    bucket_vectors = np.vstack(bucket_data[\"sbert_embedding\"].to_numpy())\n",
    "    \n",
    "    # Extract subgenre information\n",
    "    subgenres = bucket_data[\"subgenre\"].values  # Adjust column name as necessary\n",
    "    \n",
    "    # Perform clustering on bucket vectors\n",
    "    kmeans = KMeans(n_clusters=n_clusters, n_init='auto', random_state=42)\n",
    "    cluster_labels = kmeans.fit_predict(bucket_vectors)\n",
    "    \n",
    "    # Assign query to the nearest cluster\n",
    "    query_cluster = kmeans.predict(query_embedding)[0]\n",
    "    \n",
    "    perplexities = [50]\n",
    "    n_iters = [10000]\n",
    "    learning_rates = [100, 200]\n",
    "    \n",
    "    param_combinations = [(p, n, lr) for p in perplexities for n in n_iters for lr in learning_rates]\n",
    "    \n",
    "    #for perplexity, n_iter, learning_rate in param_combinations:\n",
    "    #    plot_bucket(bucket_vectors, cluster_labels, subgenres, perplexity, n_iter, learning_rate)\n",
    "        \n",
    "    # Filter beers in the same cluster as the query\n",
    "    cluster_indices = np.where(cluster_labels == query_cluster)[0]\n",
    "    cluster_vectors = bucket_vectors[cluster_indices]\n",
    "    cluster_beers = bucket_data.iloc[cluster_indices]\n",
    "    \n",
    "    # Initialize CountVectorizer with custom stopwords\n",
    "    default_stop_words = CountVectorizer(stop_words='english').get_stop_words()\n",
    "    all_stop_words = list(set(default_stop_words).union(custom_stop_words))\n",
    "\n",
    "    # Pass the combined stop words to CountVectorizer\n",
    "    vectorizer = CountVectorizer(max_features=100, stop_words=all_stop_words)\n",
    "        \n",
    "    # Extract top terms from cluster reviews\n",
    "    term_matrix = vectorizer.fit_transform(cluster_beers[\"review_text\"])\n",
    "    terms = vectorizer.get_feature_names_out()\n",
    "    term_counts = np.array(term_matrix.sum(axis=0)).flatten()\n",
    "    top_terms = [terms[i] for i in term_counts.argsort()[-5:]]  # Top 5 terms\n",
    "    \n",
    "    # Compute similarities within the selected cluster\n",
    "    sims = cosine_similarity(query_embedding, cluster_vectors)[0]\n",
    "    \n",
    "    filtered_top_terms = [term for term in top_terms if is_flavor_related(term, context_embeddings)]\n",
    "\n",
    "    # Perform collaborative filtering\n",
    "    predcicted_rating_user = collab_df.loc[user_name]\n",
    "    \n",
    "    beer_LSH = pd.DataFrame({\n",
    "        'similarity': sims,\n",
    "        'beer': cluster_beers[\"name\"].values,  # Adjust column name if necessary\n",
    "    })\n",
    "    \n",
    "    LSH_score = beer_LSH.groupby('beer')['similarity'].mean()\n",
    "    collab_filtering_scores = predcicted_rating_user[LSH_score.index.tolist()]\n",
    "\n",
    "    abv = cluster_beers.set_index(\"name\").loc[LSH_score.index, \"abv\"]\n",
    "    \n",
    "    alpha = 0.03\n",
    "    if abv_desired == 0:\n",
    "        ABV_weight = - 2 * abs(abv - abv_desired) # Ensure zero percent alchol\n",
    "    else:\n",
    "        ABV_weight =  - alpha * ((abv - abv_desired)**2) / (abv_desired**1.5 + 1)\n",
    "    \n",
    "    weighted_score = 0.85*LSH_score + 0.15*collab_filtering_scores + ABV_weight\n",
    "    \n",
    "    beer_weighted_score= pd.DataFrame({\n",
    "        'score': weighted_score,\n",
    "        'beer': cluster_beers[\"name\"].values,  # Adjust column name if necessary\n",
    "    })\n",
    "\n",
    "    return beer_weighted_score.sort_values(by='score', ascending=False), filtered_top_terms\n",
    "\n",
    "\n",
    "# Create a query\n",
    "test_query = \"I like dark beers with a sweet chocolate with a hint of cherry\"\n",
    "user_name = \"Jerseyislandbeer\"\n",
    "\n",
    "beer_recommendations, theme = recommend_beer(test_query, user_name, 7)\n",
    "\n",
    "print(\"Top 5 recommended beers:\")\n",
    "print(beer_recommendations.head())\n",
    "print(\"\\nTop flavor-related terms:\")\n",
    "print(theme)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation settup"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
