{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSH: Using cosine-similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def generate_hyperplanes(dim, num_hash_functions):\n",
    "    \"\"\"\n",
    "    Generate random hyperplanes for hash functions.\n",
    "    \n",
    "    Parameters:\n",
    "    - dim: Dimensionality of the embeddings.\n",
    "    - num_hash_functions: Number of hash functions per table.\n",
    "    \n",
    "    Returns:\n",
    "    - A matrix of shape (num_hash_functions, dim) where each row is a hyperplane.\n",
    "    \"\"\"\n",
    "    return np.random.randn(num_hash_functions, dim)\n",
    "\n",
    "def hash_vectors(vectors, hyperplanes):\n",
    "    \"\"\"\n",
    "    Hash a batch of vectors using a set of hyperplanes.\n",
    "\n",
    "    Parameters:\n",
    "    - vectors: Input vectors (2D array of shape [n_samples, d]).\n",
    "    - hyperplanes: Matrix of hyperplanes (2D array of shape [k, d]).\n",
    "\n",
    "    Returns:\n",
    "    - A matrix of binary hash values (shape [n_samples, k]).\n",
    "    \"\"\"\n",
    "    # Compute dot products and return binary hash values\n",
    "    return (np.dot(vectors, hyperplanes.T) > 0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Test hashing ###\n",
    "\n",
    "np.random.seed(42)\n",
    "num_vectors =  int(1e4) #int(10) # guess we'll have around 2 million beer reviews?\n",
    "dim = 768\n",
    "num_hash_functions = 15\n",
    "\n",
    "# Generate random vectors and hyperplanes\n",
    "test_vectors = np.random.randn(num_vectors, dim)  # 5 vectors with 768 dimensions\n",
    "test_hyperplanes = np.random.randn(num_hash_functions, dim)  # 10 hyperplanes\n",
    "\n",
    "# Test the hash_vectors function\n",
    "hash_values = hash_vectors(test_vectors, test_hyperplanes)\n",
    "hash_values\n",
    "\n",
    "# Takes about 52 seconds to hash 2 million vectors with 768 dimensions using 15 hash functions\n",
    "\n",
    "hash_keys = [tuple(h) for h in hash_values]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build LSH framework\n",
    "from collections import defaultdict\n",
    "\n",
    "class LSHVectorized:\n",
    "    def __init__(self, d, k, L):\n",
    "        \"\"\"\n",
    "        Initialize the LSH scheme with vectorized support.\n",
    "\n",
    "        Parameters:\n",
    "        - d: Dimensionality of the input vectors.\n",
    "        - k: Number of hash functions per table.\n",
    "        - L: Number of hash tables.\n",
    "        \"\"\"\n",
    "        self.L = L\n",
    "        self.tables = [defaultdict(list) for _ in range(L)]\n",
    "        self.hyperplanes = [generate_hyperplanes(d, k) for _ in range(L)]\n",
    "\n",
    "    def add_vectors(self, vectors, identifiers):\n",
    "        \"\"\"\n",
    "        Add a batch of vectors to the LSH index.\n",
    "\n",
    "        Parameters:\n",
    "        - vectors: Input vectors (2D array of shape [n_samples, d]).\n",
    "        - identifiers: A list of unique identifiers for the vectors.\n",
    "        \"\"\"\n",
    "        for table, hyperplanes in zip(self.tables, self.hyperplanes):\n",
    "            # Compute hash values for all vectors at once\n",
    "            hash_values = hash_vectors(vectors, hyperplanes)\n",
    "            \n",
    "            # Convert binary hash values to tuples for dictionary keys\n",
    "            hash_keys = [tuple(h) for h in hash_values]\n",
    "            \n",
    "            # Add vectors to their corresponding buckets\n",
    "            for identifier, key in zip(identifiers, hash_keys):\n",
    "                table[key].append(identifier)\n",
    "\n",
    "    def query(self, vectors):\n",
    "        \"\"\"\n",
    "        Query the LSH index to find similar items for a batch of vectors.\n",
    "\n",
    "        Parameters:\n",
    "        - vectors: Query vectors (2D array of shape [n_samples, d]).\n",
    "\n",
    "        Returns:\n",
    "        - A list of sets, where each set contains the candidates for a query vector.\n",
    "        \"\"\"\n",
    "        candidates = [set() for _ in range(len(vectors))]\n",
    "        for table, hyperplanes in zip(self.tables, self.hyperplanes):\n",
    "            # Compute hash values for all query vectors\n",
    "            hash_values = hash_vectors(vectors, hyperplanes)\n",
    "            \n",
    "            # Convert binary hash values to tuples for dictionary keys\n",
    "            hash_keys = [tuple(h) for h in hash_values]\n",
    "            \n",
    "            # Retrieve candidates for each query\n",
    "            for i, key in enumerate(hash_keys):\n",
    "                candidates[i].update(table.get(key, []))\n",
    "        return candidates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Candidates for query 0: {'beer_0'}\n",
      "Candidates for query 1: {'beer_1'}\n",
      "Candidates for query 2: {'beer_62', 'beer_2'}\n",
      "Candidates for query 3: {'beer_84', 'beer_72', 'beer_3'}\n",
      "Candidates for query 4: {'beer_99', 'beer_4'}\n"
     ]
    }
   ],
   "source": [
    "### Test LSH framework ###\n",
    "# Parameters\n",
    "d = 768  # Dimensionality of BERT embeddings\n",
    "k = 10   # Number of hash functions per table\n",
    "L = 5    # Number of hash tables\n",
    "\n",
    "# Initialize vectorized LSH\n",
    "lsh = LSHVectorized(d, k, L)\n",
    "\n",
    "# Generate random embeddings and IDs\n",
    "np.random.seed(42)\n",
    "embeddings = np.random.randn(100, d)  # 100 random embeddings\n",
    "ids = [f\"beer_{i}\" for i in range(100)]  # Unique IDs\n",
    "\n",
    "# Add all embeddings to the LSH index in one batch\n",
    "lsh.add_vectors(embeddings, ids)\n",
    "\n",
    "# Query multiple embeddings\n",
    "query_embeddings = embeddings[:5]  # First 5 embeddings as queries\n",
    "candidates = lsh.query(query_embeddings)\n",
    "\n",
    "# Print candidates for each query\n",
    "for i, query_candidates in enumerate(candidates):\n",
    "    print(f\"Candidates for query {i}: {query_candidates}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "comp_tools",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
